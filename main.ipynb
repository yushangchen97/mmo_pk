{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8dc996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as grid_spec\n",
    "from KDEpy import FFTKDE\n",
    "from tqdm.auto import tqdm\n",
    "from math import ceil\n",
    "import itertools\n",
    "import scipy\n",
    "from fast_histogram import histogram2d\n",
    "import colorcet as cc\n",
    "import matplotlib.colors as colors\n",
    "import h5py\n",
    "\n",
    "lmap = lambda func, *iterable: list(map(func, *iterable))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5ac034",
   "metadata": {},
   "source": [
    "Please download the .h5 dataset. \n",
    "\n",
    "Note: v0.1.0 refered to \"Visual Category\" as \"Datatype\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b993bc5b",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b4785",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(dataset_path, \"r\") as f:\n",
    "    dX = f[\"filters\"][...].reshape(-1, 9).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f065f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_hdf(dataset_path, \"meta\")\n",
    "df_meta[\"filter_ids\"] = df_meta[\"filter_ids\"].apply(lambda s: list(np.arange(int(s.split(\":\")[0]), 1 + int(s.split(\":\")[1]))))\n",
    "df_meta[\"Task\"] = df_meta[\"Task\"].apply(lambda s: \"Segmentation\" if \"Segmentation\" in s else s)  # v1.0.0 had unreliable labeling of Segmentation models, so we clean it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c593a670",
   "metadata": {},
   "source": [
    "## Scale the filters by absolute peak weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e60f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X):\n",
    "    den = np.abs(X).max(axis=1)\n",
    "    den = np.where(den == 0, 1, den)[:, None]\n",
    "    return X / den\n",
    "\n",
    "dX_scaled = scale(dX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6233dca6",
   "metadata": {},
   "source": [
    "## Perform PCA transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d2bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=9)\n",
    "dX_n = pca.fit_transform(dX_scaled)\n",
    "del dX_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b927c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab93d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dX_n_range = (dX_n.min(), dX_n.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9950f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.ylabel(\"cumulative explained variance ratio\")\n",
    "plt.xlabel(\"most-significant $n$ components\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f057cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pca.get_covariance())\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Principal Component\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c97dc3a",
   "metadata": {},
   "source": [
    "## Compute *subset* eigenimages and explained variance ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4631b82b",
   "metadata": {},
   "source": [
    "To compute the eigenimages of different splits, just select the filter_ids from the meta table and compute a PCA on the subset.\n",
    "As an example we select all filters with datatype \"fractals\" but you can change this to any selection you'd like. Note: in v0.1.0 \"fractals\" was called \"formula\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f954c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_mask = np.hstack(df_meta[df_meta[\"Visual Category\"] == \"fractals\"].filter_ids)\n",
    "subset_pca = PCA(n_components=9)\n",
    "subset_pca.fit(dX_scaled[filter_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9f4779",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 9, figsize=(9, 1))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.get_xaxis().set_ticks([])\n",
    "    ax.get_yaxis().set_ticks([])\n",
    "    im = ax.imshow(subset_pca.components_[i].reshape(3, 3), vmin=-1, vmax=1, cmap=\"seismic\")\n",
    "    ax.set_title(f\"$v_{i}$\", fontweight='bold', fontsize=18)\n",
    "    ax.set_xlabel(f\"{subset_pca.explained_variance_ratio_[i]:.2f}\", fontsize=18)\n",
    "    \n",
    "cb_ax = fig.add_axes([0.83, 0.15, 0.1, 0.7])    \n",
    "cb_ax.axis(\"off\")\n",
    "fig.colorbar(im, ax=cb_ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e94950",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(subset_pca.explained_variance_ratio_))\n",
    "plt.ylabel(\"cumulative explained variance ratio\")\n",
    "plt.xlabel(\"most-significant $n$ components\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef06d206",
   "metadata": {},
   "source": [
    "## Generate ridge plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d124570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_plot(X, xrange, shape, row_labels=None, col_labels=None, figsize=(40, 10)):\n",
    "    gs = grid_spec.GridSpec(*shape)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    ax_objs = []\n",
    "    for i in tqdm(range(shape[0])):\n",
    "        for j in range(shape[1]):\n",
    "\n",
    "            data = X[i][j]            \n",
    "            dx, dy = FFTKDE(kernel=\"gaussian\", bw='silverman').fit(data).evaluate()\n",
    "\n",
    "            color = f\"C{i%10}\"\n",
    "            \n",
    "            # creating new axes object\n",
    "            ax_objs.append(fig.add_subplot(gs[i:i+1, j:j+1]))\n",
    "\n",
    "            # plotting the distribution\n",
    "            ax_objs[-1].plot(dx, dy, color=\"#f0f0f0\", lw=1)\n",
    "            ax_objs[-1].fill_between(dx, dy, alpha=.7, color=color)\n",
    "\n",
    "            # setting uniform x and y lims\n",
    "            ax_objs[-1].set_xlim(*xrange)\n",
    "\n",
    "            # make background transparent\n",
    "            ax_objs[-1].patch.set_alpha(0)\n",
    "\n",
    "            # remove borders, axis ticks, and labels\n",
    "            ax_objs[-1].set_yticklabels([])\n",
    "            ax_objs[-1].set_yticks([])\n",
    "            ax_objs[-1].set_ylim([0, None])\n",
    "\n",
    "            for s in [\"top\", \"right\", \"left\", \"bottom\"]:\n",
    "                ax_objs[-1].spines[s].set_visible(False)\n",
    "            \n",
    "            if i == shape[0] - 1:\n",
    "                ax_objs[-1].tick_params(direction=\"inout\")\n",
    "                ax_objs[-1].spines[\"bottom\"].set_visible(True)\n",
    "                \n",
    "                if col_labels is not None:\n",
    "                    ax_objs[-1].set_xlabel(col_labels[j])\n",
    "            else:\n",
    "                ax_objs[-1].set_xticks([])\n",
    "                ax_objs[-1].set_xticklabels([])\n",
    "\n",
    "            if j == 0 and row_labels is not None:\n",
    "                ax_objs[-1].text(xrange[0] - 0.1, 0, row_labels[i], ha=\"right\", wrap=True, color=color)\n",
    "\n",
    "    gs.update(hspace=-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47da9a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype_distributions = df_meta.groupby(\"Visual Category\").filter_ids.apply(lambda x: dX_n[np.hstack(x)].T)\n",
    "\n",
    "figsize=(18, 1 + 0.5 * len(datatype_distributions))\n",
    "ridge_plot(datatype_distributions.values, xrange=dX_n_range, \n",
    "           shape=(len(datatype_distributions), 9), \n",
    "           row_labels=datatype_distributions.index, \n",
    "           col_labels=lmap(lambda i: f\"$c_{i}$\", range(9)), \n",
    "           figsize=figsize)\n",
    "plt.subplots_adjust(hspace=0, wspace=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d167ab03",
   "metadata": {},
   "source": [
    "## Generate KL-matrices and boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b6008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_sym(p, q):\n",
    "    return scipy.stats.entropy(p, q) + scipy.stats.entropy(q, p)\n",
    "\n",
    "def nd_kl_sym(p, q, weights=None):\n",
    "    if type(p) is not list and len(p.shape) == 1:\n",
    "        return kl_sym(p, q)\n",
    "    else:\n",
    "        return np.sum(lmap(lambda i: weights[i] * kl_sym(p[i], q[i]), range(len(p))))\n",
    "\n",
    "def get_kl_matrix(data, bins, x_range, weights=None):\n",
    "    d = len(data)\n",
    "    kl_matrix = np.zeros((d, d))\n",
    "    lookup = dict()\n",
    "    for p, q in tqdm(itertools.product(range(d), repeat=2), total=d**2):\n",
    "        if p not in lookup:\n",
    "            lookup[p] = get_nd_discrete_probability_distribution(data[p], x_range, bins)\n",
    "        if q not in lookup:\n",
    "            lookup[q] = get_nd_discrete_probability_distribution(data[q], x_range, bins)\n",
    "                \n",
    "        kl_matrix[p, q] = nd_kl_sym(lookup[p], lookup[q], weights=weights)\n",
    "    return kl_matrix\n",
    "\n",
    "def get_discrete_probability_distribution(X, _range, bins):\n",
    "    v, _ = np.histogram(X, range=_range, bins=bins, density=True)  # density will not sum to 1 but help to not underflow eps during normalization\n",
    "    v = v.astype(np.double) \n",
    "    v[v == 0] = np.finfo(np.float32).eps\n",
    "    v = v / np.sum(v)\n",
    "    return v\n",
    "\n",
    "def get_nd_discrete_probability_distribution(X, _range, bins):\n",
    "    if type(X) is not list and len(X.shape) == 1:\n",
    "        return get_discrete_probability_distribution(X, _range, bins)\n",
    "    else:\n",
    "        dims = list()\n",
    "        for x in X:\n",
    "            v = get_discrete_probability_distribution(x, _range, bins)\n",
    "            dims.append(v)\n",
    "        return np.vstack(dims)\n",
    "\n",
    "def kl_plot(s, figsize=(10, 10), ax=None, sort=True, **kwargs):\n",
    "    created = False\n",
    "    if ax is None:\n",
    "        created = True\n",
    "        plt.figure(figsize=figsize)\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    img = get_kl_matrix(s.values, **kwargs)\n",
    "    labels = s.index\n",
    "    if sort:\n",
    "        sort_index = np.argsort(np.mean(img, axis=0))\n",
    "        img = img[sort_index][:, sort_index]\n",
    "        labels = labels[sort_index]\n",
    "    \n",
    "    cim = ax.imshow(img, cmap=cc.cm[\"fire\"])\n",
    "    ax.set_xticks(range(len(s)))\n",
    "    ax.set_xticklabels(labels, rotation=90)\n",
    "    ax.set_yticks(range(len(s)))\n",
    "    ax.set_yticklabels(labels, rotation=0)\n",
    "    \n",
    "    if created:\n",
    "        plt.colorbar(cim)\n",
    "    \n",
    "    return ax, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfcb01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_distributions = df_meta.groupby(\"Task\").filter_ids.apply(lambda x: dX_n[np.hstack(x)].T)\n",
    "axes, kl_mat = kl_plot(task_distributions, x_range=dX_n_range, bins=70, weights=pca.explained_variance_ratio_, sort=True)\n",
    "del task_distributions\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4af90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype_distributions = df_meta.groupby(\"Visual Category\").filter_ids.apply(lambda x: dX_n[np.hstack(x)].T)\n",
    "axes, kl_mat = kl_plot(datatype_distributions, x_range=dX_n_range, bins=70, weights=pca.explained_variance_ratio_, sort=True)\n",
    "del datatype_distributions\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103a882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_inv_labels = [\"[0.0, 0.1]\", \"(0.1, 0.2]\", \"(0.2, 0.3]\", \"(0.3, 0.4]\", \"(0.4, 0.5]\", \"(0.5, 0.6]\", \"(0.6, 0.7]\", \"(0.7, 0.8]\", \"(0.8, 0.9]\", \"(0.9, 1.0]\"]\n",
    "agg_key = pd.cut(df_meta.index.get_level_values(\"conv_depth_norm\"), np.arange(0, 1.0+0.10, 0.10), include_lowest=True, labels=depth_inv_labels)\n",
    "depth_distributions = df_meta.groupby(agg_key).filter_ids.apply(lambda x: dX_n[np.hstack(x)].T)\n",
    "axes, kl_mat = kl_plot(depth_distributions, x_range=dX_n_range, bins=70, weights=pca.explained_variance_ratio_, sort=False)\n",
    "del depth_distributions\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05237a6d",
   "metadata": {},
   "source": [
    "To compare KL between different splits it's important to create histograms with similar range and bins for all splits. The range is the minimum and maximum over all splits (or just use dX_n_range which is the range of all coefs). We used 70 bins to represent the scaled filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c33b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "kl_values = []\n",
    "for grouping in [\"Task\", \"Visual Category\"]:\n",
    "    coef_groups = df_meta.groupby(grouping).filter_ids.apply(lambda x: dX_n[np.hstack(x)].T)\n",
    "    kl_mat = get_kl_matrix(coef_groups.values, weights=pca.explained_variance_ratio_, x_range=dX_n_range, bins=70)\n",
    "    kl_values.append(kl_mat[np.triu_indices(len(kl_mat), 1)])\n",
    "    labels.append(grouping)\n",
    "    \n",
    "plt.boxplot(kl_values, showmeans=True, vert=False)\n",
    "plt.gca().set_yticks(range(1, len(labels) + 1))\n",
    "plt.gca().set_yticklabels(labels)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Drift $D$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b144f0e",
   "metadata": {},
   "source": [
    "## Scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910197b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter(X, Y, bounds, bins, vmin=10e-6):\n",
    "    cmap = cc.cm[\"fire\"].copy()\n",
    "    cmap.set_bad(cmap.get_under())\n",
    "    \n",
    "    h = histogram2d(X, Y, range=bounds, bins=bins)\n",
    "    h = (h / h.max()).T\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.imshow(h, cmap=cmap, norm=colors.LogNorm(vmin=vmin, vmax=1))\n",
    "    ax.get_xaxis().set_ticks([])\n",
    "    ax.get_yaxis().set_ticks([])\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e89410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "axes = scatter(dX_n[:,0], dX_n[:,1], bounds=[[-3.3, 3.3], [-3.3, 3.3]], bins=512)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d74a14",
   "metadata": {},
   "source": [
    "## Filter Quality / Degeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b110ffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_quality_worker(w):\n",
    "    u, s, v_h = np.linalg.svd(w - w.mean(axis=0), full_matrices=False, compute_uv=True)\n",
    "    v = s**2 / (w.shape[0]-1)\n",
    "    \n",
    "    t = np.abs(w).max() / 100    \n",
    "    new_layer = np.ones_like(w)\n",
    "    new_layer[np.abs(w) < t] = 0\n",
    "\n",
    "    sparsity = (new_layer.sum(axis=1) == 0).sum() / (w.shape[0])\n",
    "    variance_entropy = scipy.stats.entropy(v, base=10)\n",
    "    return variance_entropy, sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_layer_weights(X, id_lists):\n",
    "    for id_list in id_lists:\n",
    "        yield X[id_list]\n",
    "\n",
    "with Pool() as pool:\n",
    "    layer_results = pool.map(layer_quality_worker, gen_layer_weights(dX, df_meta.filter_ids.values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
